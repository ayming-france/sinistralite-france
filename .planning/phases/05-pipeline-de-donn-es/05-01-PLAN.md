---
phase: 05-pipeline-de-donn-es
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - data/pipeline/refresh_data.py
autonomous: true
requirements:
  - PIPE-01
  - PIPE-02

must_haves:
  truths:
    - "Le script refresh_data.py existe dans data/pipeline/ et est exécutable avec Python 3.10+"
    - "Le script télécharge les fichiers Excel depuis ameli.fr et produit at-data.json, mp-data.json, trajet-data.json dans data/"
    - "Aucune référence à pickle, naf38 ou chemin absolu BPO n'apparaît dans le code"
  artifacts:
    - path: "data/pipeline/refresh_data.py"
      provides: "Pipeline principal : téléchargement Excel ameli.fr et génération JSON"
      contains: "OUTPUT_DIR = PIPELINE_DIR.parent"
  key_links:
    - from: "data/pipeline/refresh_data.py"
      to: "data/at-data.json"
      via: "json.dump avec ensure_ascii=False"
      pattern: "json\\.dump.*at-data"
    - from: "data/pipeline/refresh_data.py"
      to: "data/mp-data.json"
      via: "json.dump"
      pattern: "json\\.dump.*mp-data"
    - from: "data/pipeline/refresh_data.py"
      to: "data/trajet-data.json"
      via: "json.dump"
      pattern: "json\\.dump.*trajet-data"
---

<objective>
Copier et adapter refresh_data.py depuis le projet BPO pour créer un pipeline autonome de téléchargement Excel et génération JSON.

Purpose: Le dashboard consomme 3 fichiers JSON statiques. Ce script permet de les régénérer depuis les sources ameli.fr sans dépendance au projet BPO.
Output: `data/pipeline/refresh_data.py` fonctionnel, produisant les JSON dans `data/`.
</objective>

<execution_context>
@/Users/encarv/.claude/get-shit-done/workflows/execute-plan.md
@/Users/encarv/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-pipeline-de-donn-es/05-RESEARCH.md
@/Users/encarv/.claude/bpo/data/refresh_data.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Copier et adapter refresh_data.py</name>
  <files>data/pipeline/refresh_data.py</files>
  <action>
Copier `/Users/encarv/.claude/bpo/data/refresh_data.py` (1157 lignes) vers `data/pipeline/refresh_data.py`.

Appliquer les adaptations suivantes :

1. **Chemins de sortie :** Remplacer `DATA_DIR = Path(__file__).parent` par :
   ```python
   PIPELINE_DIR = Path(__file__).parent
   OUTPUT_DIR = PIPELINE_DIR.parent  # data/
   ```
   Tous les chemins JSON (AT_JSON_PATH, MP_JSON_PATH, TRAJET_JSON_PATH) pointent vers OUTPUT_DIR.
   Les fichiers Excel temporaires téléchargés restent dans PIPELINE_DIR.

2. **Supprimer pickle :** Retirer `import pickle`, supprimer toute écriture de fichiers `.pkl`. Remplacer `write_outputs()` par une fonction `write_json(data, json_path, label)` qui écrit uniquement du JSON (voir pattern dans RESEARCH.md). Ajouter `json_path.parent.mkdir(parents=True, exist_ok=True)` en sécurité.

3. **Supprimer naf38 :** Le dashboard n'utilise pas `by_naf38` (confirmé par grep). Supprimer toute logique de construction/agrégation `by_naf38` dans les fonctions build_at, build_mp, build_trajet. Cela simplifie significativement le code.

4. **Supprimer les chemins absolus BPO :** Vérifier qu'aucun chemin `/Users/encarv/` ou référence au projet BPO ne subsiste.

5. **AT 2021 secondary download :** Conserver le téléchargement secondaire de l'Excel 2021 (colonnes risk cause 20-31). Documenter dans un commentaire pourquoi c'est nécessaire.

6. **Nettoyer le code :** Retirer les imports non utilisés après suppression de pickle/naf38. Améliorer les messages de log (préfixer avec des indicateurs clairs). Conserver la structure fonctionnelle existante.

7. **PDF integration hook :** Si refresh_data.py appelle parse_pdf.py en import direct, conserver l'import mais le rendre optionnel avec try/except ImportError et un message clair. Le plan 02 adaptera parse_pdf.py séparément.
  </action>
  <verify>
    <automated>cd /Users/encarv/.claude/datagouv && python3 -c "import ast; ast.parse(open('data/pipeline/refresh_data.py').read()); print('Syntax OK')" && ! grep -q "import pickle" data/pipeline/refresh_data.py && ! grep -q "\.pkl" data/pipeline/refresh_data.py && ! grep -q "naf38" data/pipeline/refresh_data.py && ! grep -q "/Users/encarv" data/pipeline/refresh_data.py && grep -q "OUTPUT_DIR" data/pipeline/refresh_data.py && grep -q "at-data.json" data/pipeline/refresh_data.py && echo "ALL CHECKS PASSED"</automated>
    <manual>Vérifier que le script est lisible et bien structuré</manual>
  </verify>
  <done>refresh_data.py existe dans data/pipeline/, parse sans erreur de syntaxe, ne contient ni pickle ni naf38 ni chemin absolu, et ses chemins de sortie pointent vers data/</done>
</task>

</tasks>

<verification>
- `python3 -c "import ast; ast.parse(open('data/pipeline/refresh_data.py').read())"` passe
- Aucune occurrence de `pickle`, `.pkl`, `naf38`, `/Users/encarv` dans le fichier
- `OUTPUT_DIR` pointe vers le parent de `PIPELINE_DIR`
- Les 3 chemins JSON (at-data, mp-data, trajet-data) utilisent OUTPUT_DIR
</verification>

<success_criteria>
refresh_data.py est un script Python autonome dans data/pipeline/ qui, une fois exécuté, téléchargera les Excel ameli.fr et produira les 3 fichiers JSON dans data/. Le code est propre, sans artefacts BPO.
</success_criteria>

<output>
After completion, create `.planning/phases/05-pipeline-de-donn-es/05-01-SUMMARY.md`
</output>
